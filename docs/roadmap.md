## Overview

### Links

DLSlime is dedicated to supporting efficient transmission over a variety of different links, including but not limited to IBVerbs, CUDA IPC, TCP Socket, PCIE, NVShmem, Ascend (Direct), NVME-oF ...

### Transfer Engine

DLSlime provides a flexible and efficient P2P Transfer Engine, enabling AI-workload-aware customized functions such as Prefill-Decode disaggregation and checkpoint transmission.

### Collective Ops

Referring to [DeepEP](https://github.com/deeplink-org/DeepEP.git), DLSlime provides a buffer-based collective communication library that achieves ultra-low latency and SM-free collective communications.

### Torch Wrapper

To meet the heterogeneous requirements of SPMD programs such as heterogeneous pipeline parallel training, a Torch communication backend is provided.

## Transfer Engine

- IBVerbs Transfer Engine
  - ‚úÖ SendRecv Endpoint
  - ‚úÖ RDMA Read/Write Endpoint
- NVShmem
  - ‚úÖ NVShmem Context and Send/Recv Kernel
  - ‚ö° support NVShmem put and get wrapper
- TCP Socket
  - ‚úÖ zmq bootstrap
  - ‚è≥ TCP Socket transfer engine
- CUDA IPC
  - ‚úÖ support CUDAIPC Read/Write Endpoint
- PCIE
  - ‚è≥ Shared Memory transfer engine
  - ‚è≥ data offloading
- Ascend
  - ‚úÖ Ascned direct transfer engine
- OpenShmem
  - üí≠ Planning
- NVME-oF
  - üí≠ Planning
- UB Mesh
  - üí≠ Planning

## Collective Ops

- IBVerbs
  - ‚úÖ Send/Recv
  - ‚ö° M2N for attention-FFN disaggregation
  - ‚è≥ AllGather
  - ‚è≥ AllReduce
  - ‚è≥ All2All
- NVShmem
  - ‚è≥ Send/Recv
  - ‚úÖ AllGather
  - ‚è≥ AllReduce
  - ‚è≥ All2All
- CUDA IPC
  - ‚úÖ AllGather
  - ‚ö° High performance AllGather using CUDA Multi-Mem
  - ‚è≥ AllGather
  - ‚è≥ AllReduce
  - ‚è≥ All2All

## Torch Wrapper

- IBVerbs
  - ‚úÖ Send/Recv
  - ‚è≥ AllGather
  - ‚è≥ AllReduce
  - ‚è≥ All2All
